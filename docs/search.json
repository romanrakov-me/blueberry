[{"path":"/articles/NB_OutcomeWeights_AIPW_401k.html","id":"high-level-motivation","dir":"Articles","previous_headings":"","what":"High-level motivation","title":"AIPW vs. RA and IPW","text":"Augmented inverse probability weighting (AIPW) .k.. doubly robust estimator .k.. Double ML interactive regression model attractive theoretical properties fits Double ML universe Chernozhukov et al.Â (2018). high level, properties arise AIPW leveraging outcome treatment information estimating average treatment effects. contrast (single ML) regression adjustment (RA) inverse probability weighting (IPW), leave treatment outcome information table, respectively. matter practice? outcome weights derived Knaus (2024) allow compare three estimators task familiar many practitioners: covariate balancing.","code":""},{"path":"/articles/NB_OutcomeWeights_AIPW_401k.html","id":"comparison-in-the-401k-data","dir":"Articles","previous_headings":"","what":"Comparison in the 401(k) data","title":"AIPW vs. RA and IPW","text":"First, load packages set seed. Next, load 401(k) data hdm package. However, can adapt following code chunk load suitable data choice. Just make sure call outcome Y, treatment D covariates X. rest notebook run without modifications.","code":"if (!require(\"hdm\")) install.packages(\"hdm\", dependencies = TRUE); library(hdm) if (!require(\"cobalt\")) install.packages(\"cobalt\", dependencies = TRUE); library(cobalt) if (!require(\"viridis\")) install.packages(\"viridis\", dependencies = TRUE); library(viridis) if (!require(\"OutcomeWeights\")) install.packages(\"OutcomeWeights\", dependencies = TRUE); library(OutcomeWeights)  set.seed(1234) data(pension) # Find variable description if you type ?pension in console  # Treatment D = pension$p401 # Outcome Y = pension$net_tfa # Controls X = model.matrix(~ 0 + age + db + educ + fsize + hown + inc + male + marr + pira + twoearn, data = pension) var_nm = c(\"Age\",\"Benefit pension\",\"Education\",\"Family size\",\"Home owner\",\"Income\",\"Male\",\"Married\",\"IRA\",\"Two earners\") colnames(X) = var_nm  N = length(Y)"},{"path":"/articles/NB_OutcomeWeights_AIPW_401k.html","id":"aipw-and-its-weights","dir":"Articles","previous_headings":"Comparison in the 401(k) data","what":"AIPW and its weights","title":"AIPW vs. RA and IPW","text":"following run double ML honest random forest using OutcomeWeights internal implementation called dml_with_smoother() (packages store outcome smoothers required calculate outcome weights). Run AIPW:  Now, use get_outcome_weights() method extract outcome weights described paper.","code":"aipw = dml_with_smoother(Y,D,X,                         # progress=T, # uncomment to see progress                         # tune.parameters = \"all\",  # uncomment to tune hyperparameters                         estimators = \"AIPW_ATE\") summary(aipw) ##          Estimate      SE      t         p     ## AIPW-ATE  11553.5  1164.5 9.9216 < 2.2e-16 *** ## --- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 plot(aipw) omega_aipw = get_outcome_weights(aipw)"},{"path":"/articles/NB_OutcomeWeights_AIPW_401k.html","id":"ipw-weights","dir":"Articles","previous_headings":"Comparison in the 401(k) data","what":"IPW weights","title":"AIPW vs. RA and IPW","text":"dml_with_smoother objects also store required components calculate canonical IPW weights.","code":"D.hat = aipw$NuPa.hat$predictions$D.hat omega_ipw = matrix(D / D.hat - (1-D) / (1-D.hat),1) / N"},{"path":"/articles/NB_OutcomeWeights_AIPW_401k.html","id":"ra-weights","dir":"Articles","previous_headings":"Comparison in the 401(k) data","what":"RA weights","title":"AIPW vs. RA and IPW","text":"dml_with_smoother objects also store required components get RA weights rarely used easy obtain.","code":"omega_ra =  matrix(1, 1, N) %*% (aipw$NuPa.hat$smoothers$S.d1[1,,] - aipw$NuPa.hat$smoothers$S.d0[1,,]) / N"},{"path":"/articles/NB_OutcomeWeights_AIPW_401k.html","id":"check-covariate-balancing","dir":"Articles","previous_headings":"Comparison in the 401(k) data","what":"Check covariate balancing","title":"AIPW vs. RA and IPW","text":"use infrastructure cobalt package plot Standardized Mean Differences. , need flip sign untreated outcome weights make compatible cobalt framework. achieved multiplying outcome weights 2 \\times D-1.  covariate balancing AIPW least good balancing IPW RA, cases substantially better. particular, failure RA balance â€œBenefit pensionâ€ striking.","code":"threshold = 0.1  love.plot(     D ~ X,     weights = list(       \"AIPW (Double ML)\" = as.numeric(omega_aipw$omega * (2*D-1)),       \"IPW (Single ML)\" = as.numeric(omega_ipw * (2*D-1)),       \"RA (Single ML)\" = as.numeric(omega_ra * (2*D-1))     ),     thresholds = c(m = threshold),     var.order = \"unadjusted\",     binary = \"std\",     abs = TRUE,     line = TRUE,     colors = viridis(4)   )"},{"path":"/articles/NB_OutcomeWeights_AIPW_401k.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"AIPW vs. RA and IPW","text":"serve small appetizer engage OutcomeWeights package. run data, please feel free share results via channel. particular, find qualitatively different behavior. Also please share issues GitHub.","code":""},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Roman Rakov. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Rakov R (2025). blueberry: ML Algorythms Assignment 2. R package version 0.0.0.9000.","code":"@Manual{,   title = {blueberry: ML Algorythms For Assignment 2},   author = {Roman Rakov},   year = {2025},   note = {R package version 0.0.0.9000}, }"},{"path":"/index.html","id":"causalml-assignment-2-r-package","dir":"","previous_headings":"","what":"ML Algorythms For Assignment 2","title":"ML Algorythms For Assignment 2","text":"Welcome CausalML Assignment 2 R Package repository! ðŸš€ repository houses codebase second assignment Causal Machine Learning course.","code":""},{"path":"/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"ML Algorythms For Assignment 2","text":"R package serves comprehensive toolkit implementing causal machine learning techniques covered course assignment. Whether â€™re student looking dive deep causal inference methods researcher exploring intersection machine learning causality, package provides solid foundation.","code":""},{"path":"/index.html","id":"features","dir":"","previous_headings":"","what":"Features","title":"ML Algorythms For Assignment 2","text":"Causal Inference Algorithms: Implement state---art causal inference algorithms. Data Processing Utilities: Prepare data causal analysis effortlessly. Visualization Tools: Visualize causal effects model performance gain deeper insights. Documentation: Comprehensive documentation usage examples facilitate easy understanding implementation.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"ML Algorythms For Assignment 2","text":"can install package GitHub using devtools package: ```R devtools::install_github(â€œyourusername/yourrepositoryâ€)","code":""},{"path":"/reference/DR_learner.html","id":null,"dir":"Reference","previous_headings":"","what":"DR-learner â€” DR_learner","title":"DR-learner â€” DR_learner","text":"Estimates Predicts DR-learner using regression forests. estimation conducted full-sample.","code":""},{"path":"/reference/DR_learner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"DR-learner â€” DR_learner","text":"","code":"DR_learner(X, W, Y, train_idx, e)"},{"path":"/reference/DR_learner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"DR-learner â€” DR_learner","text":"X Feature matrix W Binary treatment indicator Y Response variable vector train_idx Training sample indicator: \"1\" train, \"0\" test e Propensity score: true estimated","code":""},{"path":"/reference/DR_learner.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"DR-learner â€” DR_learner","text":"Returns CATE values estimated \"train\" sample predicted \"test\" sample","code":""},{"path":[]},{"path":"/reference/R_learner.html","id":null,"dir":"Reference","previous_headings":"","what":"R-learner â€” R_learner","title":"R-learner â€” R_learner","text":"Estimates Predicts R-learner using regression forests. estimation conducted full-sample.","code":""},{"path":"/reference/R_learner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"R-learner â€” R_learner","text":"","code":"R_learner(X, W, Y, train_idx, e)"},{"path":"/reference/R_learner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"R-learner â€” R_learner","text":"X Feature matrix W Binary treatment indicator Y Response variable vector train_idx Training sample indicator: \"1\" train, \"0\" test e Propensity score: true estimated","code":""},{"path":"/reference/R_learner.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"R-learner â€” R_learner","text":"Returns CATE values estimated \"train\" sample predicted \"test\" sample","code":""},{"path":[]},{"path":"/reference/T_learner.html","id":null,"dir":"Reference","previous_headings":"","what":"T-learner â€” T_learner","title":"T-learner â€” T_learner","text":"Estimates Predicts T-learner using regression forests. estimation conducted full-sample.","code":""},{"path":"/reference/T_learner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"T-learner â€” T_learner","text":"","code":"T_learner(X, W, Y, train_idx)"},{"path":"/reference/T_learner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"T-learner â€” T_learner","text":"X Feature matrix W Binary treatment indicator Y Response variable vector train_idx Training sample indicator: \"1\" train, \"0\" test","code":""},{"path":"/reference/T_learner.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"T-learner â€” T_learner","text":"Returns CATE values estimated \"train\" sample predicted \"test\" sample","code":""},{"path":[]},{"path":"/reference/X_learner.html","id":null,"dir":"Reference","previous_headings":"","what":"X-learner â€” X_learner","title":"X-learner â€” X_learner","text":"Estimates Predicts X-learner using regression forests. estimation conducted full-sample.","code":""},{"path":"/reference/X_learner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"X-learner â€” X_learner","text":"","code":"X_learner(X, W, Y, train_idx, e)"},{"path":"/reference/X_learner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"X-learner â€” X_learner","text":"X Feature matrix W Binary treatment indicator Y Response variable vector train_idx Training sample indicator: \"1\" train, \"0\" test e Propensity score: true estimated","code":""},{"path":"/reference/X_learner.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"X-learner â€” X_learner","text":"Returns CATE values estimated \"train\" sample predicted \"test\" sample","code":""},{"path":[]},{"path":"/reference/aipw_GATE.html","id":null,"dir":"Reference","previous_headings":"","what":"Pseudo-outcomes AIPW estimation â€” aipw_GATE","title":"Pseudo-outcomes AIPW estimation â€” aipw_GATE","text":"Pseudo-outcomes AIPW estimation","code":""},{"path":"/reference/aipw_GATE.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pseudo-outcomes AIPW estimation â€” aipw_GATE","text":"","code":"aipw_GATE(X, W, Y, e, nfolds)"},{"path":"/reference/aipw_GATE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pseudo-outcomes AIPW estimation â€” aipw_GATE","text":"X Feature matrix W Binary treatment indicator Y Response variable vector e Propensity score: true estimated nfolds Numbers folds cross-fitting","code":""},{"path":"/reference/aipw_GATE.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pseudo-outcomes AIPW estimation â€” aipw_GATE","text":"Returns vector pseudo-outcomes","code":""},{"path":[]},{"path":"/reference/monte_carlo.html","id":null,"dir":"Reference","previous_headings":"","what":"Monte Carlo draws for coverage â€” monte_carlo","title":"Monte Carlo draws for coverage â€” monte_carlo","text":"function bit challenging. also takes long time run. use analyse results depend seed number well train/test shares.","code":""},{"path":"/reference/monte_carlo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Monte Carlo draws for coverage â€” monte_carlo","text":"","code":"monte_carlo(X, W, Y, e, iterations, train_share)"},{"path":"/reference/monte_carlo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Monte Carlo draws for coverage â€” monte_carlo","text":"X Feature matrix W Binary treatment indicator Y Response variable vector e Propensity score: true estimated iterations Number times rerun analysis train_share Share training sample relative full data","code":""},{"path":"/reference/monte_carlo.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Monte Carlo draws for coverage â€” monte_carlo","text":"Returns shares confidence intervals estimated HTE b2 cover zero.","code":""},{"path":[]}]
